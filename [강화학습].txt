[강화학습]

강화 학습은 기계 학습의 강력한 분기입니다. 시간 t까지 관찰된 데이터가 시간 t + 1에서 취할 행동을 결정하는 것으로 간주되는 상호 작용 문제를 해결하는 데 사용됩니다. 또한 기계가 걷기와 같은 작업을 수행하도록 훈련할 때 인공 지능에 사용됩니다. 원하는 결과는 AI에게 보상을 제공하고 바람직하지 않은 처벌을 제공합니다. 기계는 시행착오를 통해 학습합니다.

1. 신뢰 상한(UCB)
Multi-Armed Bandit Problem(멀티 암드 밴딧 문제)
이 예시는 강화 학습 섹션 전체를 사용됩니다.

강화학습을 예로 들면, 로봇 강아지가 걸을 수 있게 훈련 시키는 데도 강화 학습이 사용됩니다.
로봇 강아지를 만들면 그 안에 로봇 강아지에게 걷는 법을 알려주는 알고리즘을 구현해야 합니다.
오른쪽 앞발을 움직여, 왼쪽 뒷발을 움직여, 왼쪽 앞발, 오른쪽 뒷발 등등 이런 식으로요.
걷기 같은 임무를 수행하기 위해 필요한 연속적인 행동을 줄 수도 있고 정말 재미있는 방식으로 강아지가 걷는 것을 훈련시키는 강화 학습 알고리즘을 구현할 수도 있죠.
이게 하는 일은 강아지에게 이렇게 말하는 거예요. "여기 네가 취할 수 있는 행동들이 있단다.", "다리를 이렇게 움직여 봐. 이렇게 움직일 수 있어.", "네 목표는 앞으로 나아가는 거야", "앞으로 나갈 때마다 상을 받는단다", "넘어 질 때는 벌을 받는단다" 이렇게요.
보상은 알고리즘 안에 있어요. 알고리즘에서 1은 보상 0은 벌이에요. 그래서 이게 무작위로 일련의 행동을 할 겁니다. 그리고 어떤 행동이 그들을 앞으로 걷게 했는지를 보는 거죠. 그 행동들이 좋았는지 보고 그걸 계속 반복하는 거예요. 그렇게 해서 강아지들이 걷는 법을 배우게 되죠. 걷기 알고리즘을 프로그래밍해서 넣을 필요가 없어요. 스스로 걷는 법을 찾아낼 거니까요.

이 섹션에서는 멀티 암드 밴딧 문제를 다룰 거예요. 강화 학습의 머신러닝 부분의 다른 응용입니다. 강화 학습의 다른 응용도 정말 많아요.

멀티 암드 밴딧이 뭘까요?
막 떠오르는 건 도둑이 은행 안으로 들어가는 거죠. 아니면 총을 든 강도나 한 팔만 있는 강도요.
원 암드 밴딧은 슬롯 머신입니다. 그럼 왜 원 암드 밴딧라고 불릴까요? 과거에는 오른쪽에 이 손잡이가 있었거든요. 영화에서 볼 수 있거나 드물게 손잡이를 당겨야하는 이런 슬롯 머신을 찾을 수 있는 곳들이 있어요. 왜냐하면 요즘은 다 전자화가 돼서 그냥 버튼을 누르면 되거든요. 버튼을 누르는 방식의 슬롯 머신들이죠. 하지만 과거에는 작동하거나 게임을 초기화할 때 레버를 당겨야 했어요. 그래서 팔이라고 하는 겁니다.

하지만 왜 강도이라고 불리는 걸까요? 이 머신들은 카지노에서 돈을 잃는 가장 빠른 방법 중 하나이기 때문입니다. 과거에는 돈을 잃는 확률이 50% 정도였던 것 같아요. 그러니까 이기는 것보다 적게 버는 거죠. 돈을 따거나 잃는 확률이 딱 50대 50 정도였던 것 같아요. 하지만 그들이 버그를 넣은 거예요. 인터넷에서 읽을 수 있는데 버그를 넣어서 사람들이 더 빨리 지게끔 만든 거죠 심지어는 50% 보다 더 자주요. 그러므로 이 밴딧라는 이름은 이렇게 돈을 빼앗아 가서 붙여진 이름이에요. 돈을 잃는 가장 빠른 길이죠. 그냥 돈을 다 내주는 거예요.
그래서 이게 원 암드 밴딧이라고 불리는데 (One-Armed Bandit)

그럼 대체 멀티 암드 밴딧은 뭘까요?
멀티 암드 밴딧 문제는 이 머신들이 여러 개가 있을 때 마주하는 과제입니다. 하나가 아니라 다섯 개나 열 개 정도가 있는 거죠 우리 프로그래밍 예제에서는 열 개의 예제가 있는 거예요. 이게 비록 멀티 암드 밴딧 문제라고는 불리지만 정말 많은 응용이 있다는 걸 알게 되실 거예요. 정말 많은 문제를 해결하는데 사용되기도 합니다.

여기 과제가 있습니다.이 다섯 개의 머신이 있어요.
그리고 여러분이 실행할 수 있는 게임의 수에서 최대로 돌려받으려고 한다면 어떻게 게임을 하시겠어요? 백 번, 천 번의 게임을 하기로 결정했다고 해봅시다 그리고 거기서 최대로 돌려받고 싶다고요. 그 보상을 최대화하려면 어떤 머신으로 게임을 해야 하는지 어떻게 알아내실 건가요? 문제를 더 자세히 설명하기 위해서 각 머신 뒤에는 분포가 있다는 가정을 말씀드리겠습니다. 머신이 결과로 선택하는 숫자나 결과가 분포되어 있어요. 그래서 각각의 머신에 그 분포가 있고 그 안에서 결과를 고르는 거죠. 트리거를 당기면 그 분포 안에서 무작위로 결과를 가져오는 겁니다. 여러분이 이기거나 졌거나, 얼마를 벌었거나, 얼마를 잃거나 이런거요. 또는 동전을 넣은만큼 잃게 되는 거죠.하지만 이건 머신 안에 만들어진 분포에 기반해 여러분이 이기고 지고를 결정하는 거예요.
문제는 여러분은 이 분포를 모른다는 거죠.어떻게 분포되어 있는지 미리 알 수가 없어요. 그리고 이 머신마다 다를 것으로 간주되죠. 가끔은 이 머신들 이 비슷하거나 똑같기도 해요 하지만 기본적으로 다 다릅니다.여러분의 목표는 이 중에 여러분에게 가장 좋은 분포 방식을 가진 것을 찾아내는 거예요.
다섯 대의 머신이 있으니 여기 다섯 개의 분포가 있습니다. 이걸로 뭐가 가장 좋은 머신이냐고 하면 당연히 맨 오른쪽 머신이죠. 주황색이 가장 좋은 머신이에요. 이게 여러분한테 가장 좋아요. 이게 가장 {Left-skewed} 거든요 꼬리가 왼쪽에 길게 있어요. 그래서 이게 가장 이익을 많이 주는 결과를 가져올 거예요 가장 높은 평균, 중위값, 최빈값을 가지거든요. 만약 여러분이 이 분포를 안다면 당연히 다섯 번째 머신으로 가서 돈을 걸겠죠. 항상 딱 이 다섯 번째 머신에만요 왜냐하면 이게 가장 좋은 분포를 가지니가요. 평균적으로 가장 좋은 결과를 얻게 됩니다 하지만 미리 알 수 있는 방법이 없죠.

그래서 여러분의 목표는 이걸 알아내는 거예요. 심리 작전 같은 거죠. 어떤 가장 좋은지 모릅니다. 그걸 찾아내야 해요. 하지만 동시에 이걸 알아내면서 계속 비용이 발생하거든요. 그러니 알아내는데 오래 걸리면 손해가 커지는 거예요. 알아내는 시간이 길어질수록 틀린 데다 돈을 더 쓰게 되는 거죠. 그러므로 매우 빠르게 알아내야 해요. 이 안에는 두 가지 요소가 있는데 {exploration}과 {exploitation}이에요.

exploration: 탐색은 새로운 지식, 정보, 자원, 영역 또는 경험을 조사하거나 검색하는 행위를 말합니다.
exploitation: 착취는 종종 불공평하거나 비윤리적인 방식으로 개인의 이익을 위해 누군가 또는 무언가를 이용하는 행위를 말합니다.

일단 어떤 머신이 가장 좋은지 {explore}해야 합니다. 그리고 동시에 {exploitng}을 시작하는 겁니다. 이 머신들을 개발하고 최대 보상을 만들 방법을 개발하는 거죠. 이 뒤에는 후회 이론이라는 또 다른 수학 개념이 있어요 후회가 수학적으로 정의된 거죠. 이 후회는 대체 불가능하거나 최상이 아닌 메소드를 사용할 때 주로 생깁니다.

후회 이론에 대해 더 읽어보고 싶으시다면 추천해드릴 좋은 논문이 있어요. [Using Confidence, Bounds, for Exploitation and Exploration or tradeoffs]가 제목입니다. 오스트리아에 있는 기술대의 대학원생인 피처 아우어가 쓴 논문이에요. 더 자세히 알아보고 싶으시면 처음 두어 챕터를 읽어보시면 돼요. 

최상이 아닌 머신을 사용할 때는 {regret}이 정량화될 수 있어요. 예를 들면 최상의 결과와 최상이 아닌 결과의 차이나 다른 머신들을 탐험하는데 사용되는 비용, 즉 기회 비용 같은 것의 합계를 의미하죠. 그러니까 최상이 아닌 머신을 더 길게 탐험할 수록 {regret}은 높아지는 거예요. 동시에 충분히 탐험하지 않으면 충분히 긴 시간동안 탐험하지 않으면 최상에 못 미치는 머신이 최상의 머신으로 나타나게 됩니다.

그래서 목표는 최상을 찾아서 개발하는 겁니다. 하지만 이 모든 것을 탐험하는 데는 최소한의 시간을 투자해야 해요. 여러분이 탐험하는 동안 돈을 얻기도 합니다. 하지만 최상의 머신에서 얻는 건 아니죠. 그래서 그게 목표예요.그게 이 전체 과제의 요점입니다.

현대에서 이것이 가장 흔하게 응용되는 분야는 우리도 탐험해 볼 광고 분야입니다. 몇 가지 광고를 볼까요? 재미있을 거예요. 
코카 콜라나 어떤 회사가 캠페인을 한다고 가정해 봅시다. "코크의 생활에 오신 것을 환영합니다" 캠페인이에요. 인터넷에 이 캠페인을 검색하면 이 캠페인을 위한 수백 개의 다른 광고가 나올 거예요. 이게 하나의 예시입니다. 이건 그냥 구글에서 찾아온 거라 그냥 사람들이 그린 걸 수도 있어요. 하지만 이게 캠페인에 사용되는 정식 광고라고 가정합시다. 그리고 최상의 광고를 찾는 거예요. 확실히 효과가 있는 광고요. 다섯 개의 후보가 있습니다. 그리고 우리 목표는 어떤 광고가 최상의 효과를 내느냐예요 최대의 보상을 가져오는 거죠. 하지만 지금은 어떤 게 가장 효과가 있을지 모릅니다. 이 뒤엔 당연히 분포가 있죠.하지만 이 분포라는 건 수 천명의 사람이 이걸 보고 클릭하고 그 이후에야 알 수 있어요. 프로그래밍 강의에서 보여드릴 예시에는 광고가 열 개에서 그 이상으로 있어요. 그래서 여기서 어떻게 해야 될까요?
접근법 중 하나는 AB 테스트를 해보는 거죠. 5 또는 50 또는 500 개의 광고를 가져다가 큰 AB 테스트 또는 여러 개의 AB 테스트를 하는 거예요. 샘플이 충분히 많아질 때까지 기다렸다가 어떤 광고가 확실한 신뢰도와 함께 가장 좋은 비율을 가지는지 보는 거죠. 문제는 그렇게 하면 엄청난 돈과 시간을 들이게 된다는 거예요. 그래서 AB 테스트는 순수한 탐험인 겁니다. 최상의 옵션을 개발하는 게 아니에요. 최상의 옵션을 개발하는 거긴 하죠 그런데 어느 정도는 최상까지는 아닌 옵션들을 개발하는 겁니다. 이전의 분포를 생각해보면 이게 최상이죠 AB 테스트를 진행 해보고 똑같이 분포한다거나 똑같이 이 다섯 개의 옵션을 사용한다고 하면 이걸 쓰는 만큼 나머지 네 개도 쓸 거란 말이죠. 그러니까 다섯 개를 모두 사용하는 거예요. 이게 개발되기도 하겠지만 그건 정말 우연히 그런 거죠. 그래서 AB 테스트는 그냥 탐험이라는 겁니다. 
그래서 도전 과제는 어떤 게 최상인지를 찾는 거예요. 하지만 탐험하는 와중에 최상의 것을 개발해야 해요. 그러니 이 과정에서 최상을 찾으세요. 이 실제 캠페인의 과정에서 찾는다고 생각하세요. 두 단계를 거칠 필요는 없어요. AB 테스트를 해서 최상의 것을 찾으세요. 하지만 가장 빠른 방법으로 최상의 것을 찾아서 그 와중에 개발하기 시작하세요. 이게 우리가 해결할 과제입니다. 그리고 멀티 암드 밴딧 문제의 현대적 응용이에요.

----------------------------------------------------------------------------------------------

오늘은 {Upper Confidence Bound}에 대해 알아보고 머신러닝의 강화 학습의 일부인 이 알고리즘의 뒤에 있는 직관에 대해 알아보겠습니다. 

이전에 이야기한 것처럼 우리가 해결해야 할 문제는 멀티 암드 밴딧 문제입니다. 다섯 대나, 그 이상의 슬롯 머신이 있고 그 중 아무데나 돈을 걸 수 있죠. 그리고 보상을 어떻게 최대화하려면 어떻게 걸어야 할지를 찾는 겁니다. 각 머신에는 특정 분포가 있다고 했었죠. 그리고 어떤 분포가 최상인지 모르기 때문에 이 머신들의 {exploration}과 {exploitation}을 조합해서 어떤 머신이 최상인지 찾아야 합니다. 그리고 그걸 개발하는 거죠. 이 문제의 현대적인 응용은 바로 광고입니다. 만약 5, 10, 50 또는 500 편의 다른 광고가 있다고 하면 어떤 게 최상의 효과를 내는지 어떻게 찾으실 건가요? 단순히 AB 테스트를 실행하고 결과를 볼 수도 있죠. 하지만 그건 탐험을 하고 개발은 따로 하겠다는 의미예요. 돈도 많이 들고 시간도 많이 낭비하게 되죠. 우리는 그 둘의 조합을 원해요. 그래서 최상의 결과를 빠르게 가져오고 우리 노력의 결과를 최대화하는 거죠. 멀티 암드 밴딧의 요약입니다.

빠르게 훑어보고 재미있는 과정으로 넘어갈게요

{d arm}이 있습니다. 예를 들면 {arm}들은 사용자가 웹페이지에 접속할 때마다 보여주는 광고예요. 사용자가 접속할 때마다 광고가 표시되는데 각 라운드마다 어떤 광고를 표시할지 선택하는 겁니다. 하나의 암 밴딧으로 딱 하나의 광고를 표시할 수 있어요. 딱 하나의 머신만 고르는 거죠. 그리고 각 라운드에서 광고 {i}가 보상으로 0이나 1을 반환합니다. 만약 사용자가 광고를 클릭하면 r_i(n)=1이 되고 클릭하지 않으면 0이 됩니다. 우리의 목표는 미디어에서 얻는 총 보상을 최대화 하는 겁니다. 이게 우리가 할 일이에요. 

그리고 이게 UCB 알고리즘이 작동하는 방식입니다. 알고리즘의 필수적인 부분만 볼게요. 직관 파트로 넘어가 봅시다 이게 어떻게 작동할까요? 알고리즘이 실행될 때 백그라운드에서는 무슨 일이 일어나고 있나요?
이게 우리의 슬롯머신 또는 원 암드 밴딧이에요. 그리고 각 슬롯머신에 일정 분포가 있습니다. 우리는 최상의 머신을 찾을 거예요. 그냥 딱 보면 뭐가 제일 좋은지 알 수 없죠. 하지만 우리는 알아요. 결과를 볼게요. 어떻게 생각하세요 ? 이게 분포예요. 이 머신들 뒤에 있는 분포죠. 이 분포가 그들이 결과를 내는 방식이거든요. 이렇게 보면 어떤 게 최상의 머신인지 알 수 있죠. 만약 여러분이 게임을 하고 있다면 돈을 어디에 계속 거시겠어요? 주황색이죠. 이게 가장 최상의 결과를 반환하는 걸 알 수 있습니다. 그리고 여러분은 항상 여기에 돈을 걸고 싶을 거예요. 그러면 결과가 최상이니까요. 하지만 우리는 이걸 모르죠. 그리고 이 머신들로 게임을 하는 과정에서 찾아내는 거예요. 광고도 마찬가지고요. 광고도 내보내는 와중에 어떤 게 클릭 수가 높은지를 찾아내는 겁니다. 실제 캠페인을 내보내기 전에 탐험과 개발을 할 시간과 비용은 없어요. 
그래서 그 과정에서 찾고 싶은 거예요. 시작부터 돌아오는 이익을 최대화하고 싶은 거죠. 

어떻게 하면 될까요? 이 분포를 실제 예상 이익으로 변환해 봅시다. 이 분포를 수직축으로 옮길게요. 이게 수직축입니다. 각 머신의 각 분포에 대한 예상 값/이익입니다. 이게 우리 y축이에요. 하지만 아직도 우리는 몰라요. 대체 이 알고리즘이 어떻게 작동하는 걸까요? 일단 이건 모든 분포의 시작점을 가정합니다. 분명히 시작값이 있다고 가정하는 거예요. 이 머신이 다 똑같기 때문에 그 사이에서 구분해 낼 수는 없어요. 그러니 다 같은 이익이 돌아온다고 가정해 봅시다. 그리고 그 수준에 놓아줄게요. 그럼 알고리즘은 이 알고리즘 뒤에 이 공식을 만듭니다. 컨피던스 밴드를 만듭니다. 이것은 확실성의 매우 높은 수준으로 만들어 져요. 이 컨피던스 밴드는 실제로 돌아오는 이익이나 실제로 예상되는 이익을 포함합니다. 처음 두어 번의 라운드는 그냥 시험이 될 겁니다. 의도적으로 최소 한 번은 시도해보는 거죠. 아 값을 여기에 배치하기 위해서요. 처음에는 이 신뢰 구간이 정말 큽니다. 아 값을 여기에 배치하기 위해서요 처음에는 이 신뢰 구간이 정말 큽니다. 매우 크지만 이건 구체적으로 예상 값에 밎춰 만들어 졌어요. 여기 보이는 게 그 값입니다 매우 높은 수준의 신뢰도가 이 신뢰도 수준으로 떨어져요. 매우 높은 수준의 확실성이 이 신뢰도 안으로 떨어집니다. 우리가 만들어낸 이 빨간 실험값으로요. 매우 높은 수준의 확실성이 이 신뢰도 안으로 떨어집니다. 우리가 만들어낸 이 빨간 실험값으로요. 맨 처음에는 모두 똑같습니다. 그러면 대체 이 알고리즘이 어떻게 작동하는 걸까요?

이 중에 가장 높은 신뢰도를 가진 머신을 고르는 거예요. 하지만 지금은 이 중 어떤 머신인지 알 수 없죠. 지금은 모두 같은 신뢰도를 가지고 있어요. 그래서 이 알고리즘이 UCB라고 불리는 겁니다. 일단 이 중 하나를 골라볼게요. 어떤 걸 골라도 상관 없으니까요. 이 컬러 라인에 대해서는 알 수 없어요. 여기 있는 것 모두 알 수 없습니다. 우리가 이걸 분석하는 입장에서 우리는 이 박스만 보여요. 다 똑같이 보이는 거예요. 그러니 아무거나 일단 고릅니다. 이걸 골라보죠. 그 다음에는 머신의 레버를 당기는 거예요. 광고를 게시하는 겁니다. 광고를 게시한 다음 누군가가 그 광고를 클릭했는지 클릭하지 않았는지 봐야죠. 이 경우 광고를 클릭하지 않은 겁니다. 그러면 이 붉은값이 내려갑니다. 왜냐하면 오직 이 머신에만 다른 관측값이 생기면 이 머신의 관측값 전체 샘플에 더해지거든요.이 붉은값이 관측 평균이기 때문에 이 값이 내려가는 겁니다. 관측 평균은 큰 수의 법칙에 따라 결국에는 이 분포의 예상된 이익 또는 예상된 평균 또는 예상된 값으로 모이게 됩니다. 그러므로 이 값이 내려갈 확률이 높은 거죠. 그리고 추가적인 관측값이 생겼기 때문에 두 번째로 변화를 보이는 건 신뢰 구간입니다. 
신뢰 구간이 더 작아진 게 보이시죠? 추가적인 기간이 생겨서 그래요. 당연히 그렇게 큰 변화는 아닙니다. 하지만 요점만 말하자면 우리가 추가 관측값이 생겼기 때문에 우리 예측에 대한 확신이 생긴 겁니다. 일어나는 모든 일에 대해 더욱 확신이 생긴 거예요. 그래서 신뢰 구간이 서서히 줄어들기 시작하는 거죠. (~ 과정 반복) 최상의 것을 계속해서 개발해 감으로써 이게 최상이라는 것을 알 수 있습니다.

이게 이 UCB 알고리즘이 멀티 암드 밴딧 문제를 해결하는 방법입니다. 굉장히 흥미로운 해결법이에요. 그냥 무작위로 고르거나 AB 테스트를 해보고 옵션을 고르는 것보다 정교하죠. 

2. 톰슨 샘플링